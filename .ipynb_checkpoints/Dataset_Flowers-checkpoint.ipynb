{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt \n",
    "import glob\n",
    "from skimage.io import imread\n",
    "\n",
    "daisy,dandelion,rose,sunflower,tulip = [],[],[],[],[]\n",
    "\n",
    "for filename in glob.glob('flowerscomp2/daisy/*.jpg'): # for daisies\n",
    "    im = imread(filename,True).flatten()\n",
    "    daisy.append(im)    \n",
    "\n",
    "for filename in glob.glob('flowerscomp2/dandelion/*.jpg'): # for dandelion\n",
    "    im = imread(filename,True).flatten()\n",
    "    dandelion.append(im)\n",
    "    \n",
    "for filename in glob.glob('flowerscomp2/rose/*.jpg'): # for roses\n",
    "    im = imread(filename,True).flatten()\n",
    "    rose.append(im)\n",
    "    \n",
    "for filename in glob.glob('flowerscomp2/sunflower/*.jpg'): # for sunflowers\n",
    "    im = imread(filename,True).flatten()\n",
    "    sunflower.append(im)\n",
    "    \n",
    "for filename in glob.glob('flowerscomp2/tulip/*.jpg'): # for tulip\n",
    "    im = imread(filename,True).flatten()\n",
    "    tulip.append(im)\n",
    "    \n",
    "daisy_arr=np.asarray(daisy)\n",
    "dandelion_arr=np.asarray(dandelion)\n",
    "rose_arr=np.asarray(rose)\n",
    "sunflower_arr=np.asarray(sunflower)\n",
    "tulip_arr=np.asarray(tulip)\n",
    "\n",
    "# daisy = 0, dandelion = 1, rose = 2, sunflower = 3, tulip = 4\n",
    "daisy_labels=np.zeros((len(daisy_arr),1))\n",
    "dandelion_labels=np.ones((len(dandelion_arr),1))\n",
    "rose_labels=2*np.ones((len(rose_arr),1))\n",
    "sunflower_labels=3*np.ones((len(sunflower_arr),1))\n",
    "tulip_labels=4*np.ones((len(tulip_arr),1))\n",
    "\n",
    "\n",
    "# Here are the labels and the dataset \n",
    "labels=np.vstack((daisy_labels,dandelion_labels,rose_labels,sunflower_labels,tulip_labels))\n",
    "dataset=np.vstack((daisy_arr,dandelion_arr,rose_arr,sunflower_arr,tulip_arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(A):\n",
    "    stds = [np.std(A[r,:]) for r in range(A.shape[0])]\n",
    "    for r in range(A.shape[0]):\n",
    "        A[r,:] /= stds[r]\n",
    "    return A\n",
    "\n",
    "def centralize(A):\n",
    "    means = [np.mean(A[r,:]) for r in range(A.shape[0])]\n",
    "    for r in range(A.shape[0]):\n",
    "        A[r,:] -= means[r]\n",
    "    return A\n",
    "\n",
    "def unisonShuffledCopies(a, b):\n",
    "    assert len(a) == len(b)\n",
    "    p = np.random.permutation(len(a))\n",
    "    return a[p], b[p]\n",
    "\n",
    "def PCA(A, numComponents):\n",
    "    # First, center the data\n",
    "    A = normalize(centralize(A))\n",
    "    \n",
    "    # Now, compute the SVD, and reduce the dimensions of A.\n",
    "    U, Sigma, Vt = np.linalg.svd(A)\n",
    "    D = np.matmul(A.T, U[:,:numComponents])\n",
    "    \n",
    "    return D.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# means of the photos\n",
    "fig=plt.figure(figsize=(20, 20))\n",
    "fig.add_subplot(1,5,1)\n",
    "plt.imshow(np.reshape(daisy_arr.mean(0),(100,100)),cmap='bone')\n",
    "plt.title(\"Daisy, mean\")\n",
    "fig.add_subplot(1,5,2)\n",
    "plt.imshow(np.reshape(dandelion_arr.mean(0),(100,100)),cmap='bone')\n",
    "plt.title(\"Dandelion, mean\")\n",
    "fig.add_subplot(1,5,3)\n",
    "plt.imshow(np.reshape(rose_arr.mean(0),(100,100)),cmap='bone')\n",
    "plt.title(\"Rose, mean\")\n",
    "fig.add_subplot(1,5,4)\n",
    "plt.imshow(np.reshape(sunflower_arr.mean(0),(100,100)),cmap='bone')\n",
    "plt.title(\"Sunflower, mean\")\n",
    "fig.add_subplot(1,5,5)\n",
    "plt.imshow(np.reshape(tulip_arr.mean(0),(100,100)),cmap='bone')\n",
    "plt.title(\"Tulip, mean\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing top principal components \n",
    "fig=plt.figure(figsize=(8,8))\n",
    "centered_daisy = daisy_arr.T-np.reshape(daisy_arr.T.mean(1),(10000,1))\n",
    "centered_dandelion = dandelion_arr.T-np.reshape(dandelion_arr.T.mean(1),(10000,1))\n",
    "u1,s1,v1 = np.linalg.svd(centered_daisy)\n",
    "u2,s2,v2 = np.linalg.svd(centered_dandelion)\n",
    "component1,component2 = u1[:,[0]],u1[:,[1]]\n",
    "component3,component4 = u2[:,[0]],u2[:,[1]]\n",
    "fig.add_subplot(221)\n",
    "plt.imshow(np.reshape(component1,(100,100)),cmap='bone')\n",
    "plt.title(\"Daisy, first component\")\n",
    "fig.add_subplot(222)\n",
    "plt.imshow(np.reshape(component2,(100,100)),cmap='bone')\n",
    "plt.title(\"Daisy, second component\");\n",
    "fig.add_subplot(223)\n",
    "plt.imshow(np.reshape((component3),(100,100)),cmap='bone')\n",
    "plt.title(\"Dandelion, first component\")\n",
    "fig.add_subplot(224)\n",
    "plt.imshow(np.reshape((component4),(100,100)),cmap='bone')\n",
    "plt.title(\"Dandelion, second component\");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Centering the dataset \n",
    "mean=np.reshape(dataset.T.mean(1),(len(dataset.T),1))\n",
    "centered=dataset.T-mean\n",
    "\n",
    "i = 0\n",
    "stds = [np.std(row) for row in centered]\n",
    "normalized_samples=[]\n",
    "for row in centered:\n",
    "    normalized_samples.append(row/stds[i])\n",
    "    i+=1\n",
    "\n",
    "# Here I used centered to mean centered and normalized\n",
    "centered = np.asarray(normalized_samples)\n",
    "\n",
    "U,S,V=np.linalg.svd(centered)\n",
    "\n",
    "# Determining Variation\n",
    "total_s = 0\n",
    "fnorm = np.linalg.norm(centered,'fro')\n",
    "for k in range(2,12):\n",
    "    total_s += S[k]**2\n",
    "    print(k,(total_s/fnorm**2)*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualization of the datapoints \n",
    "\n",
    "\n",
    "p2c = (U[:,[0,1]].T@centered).T\n",
    "points = [p2c[0:768],p2c[768:1820],p2c[1820:2603],p2c[2603:3337],p2c[3337:4321]]\n",
    "colors = ['r','b','g','k','m']\n",
    "which_flower = ['daisy','dandelion','rose','sunflower','tulip']\n",
    "fig = plt.figure()\n",
    "\n",
    "for i in range(5):\n",
    "    x,y = points[i][:,0], points[i][:,1]\n",
    "    plt.scatter(x,y,c=colors[i],label=which_flower[i])\n",
    "plt.title('Two Dimensional PCA Features')\n",
    "plt.legend(loc='upper right');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-Nearest Neighbors (Bill Lee)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from heapq import heappush, heappop\n",
    "from scipy import stats\n",
    "\n",
    "def kNN(k,testset,testlabels,trainset,trainlabels):\n",
    "    # distance stores threeple (euclidan distance, training class, actual test class)\n",
    "    confusion = np.zeros((5,5))\n",
    "    for i in range(len(testset)):\n",
    "        distances = []\n",
    "        for j in range(len(trainset)):\n",
    "            heappush(distances,(np.linalg.norm(testset[i]-trainset[j]),trainlabels[j]))\n",
    "    \n",
    "        # now take the k nearest\n",
    "        tally = []\n",
    "        for near in range(k):\n",
    "            tally.append(heappop(distances)[1])\n",
    "        \n",
    "        winner = int(stats.mode(tally)[0])\n",
    "        confusion[int(testlabels[i])][winner] += 1\n",
    "    \n",
    "    return confusion\n",
    "        \n",
    "\n",
    "def fiveFoldCV(data,labels):\n",
    "    mega_confusion=[]\n",
    "    for k in range(5,10,2):\n",
    "        total_confusion = np.zeros((5,5))\n",
    "        for i in range(5):\n",
    "            sindex1 = (i*864)\n",
    "            sindex2 = 4321 if i==4 else (i+1)*864\n",
    "            test_set = data[sindex1:sindex2]\n",
    "            test_labels = labels[sindex1:sindex2]\n",
    "            training_set = np.concatenate((data[sindex2:4321],data[0:sindex1]))\n",
    "            training_labels = np.concatenate((labels[sindex2:4321],labels[0:sindex1]))\n",
    "            temp_confusion = kNN(k, test_set, test_labels, training_set, training_labels)\n",
    "            total_confusion += temp_confusion\n",
    "        mega_confusion.append(total_confusion)\n",
    "        \n",
    "    return mega_confusion\n",
    " \n",
    "mega_confusion = fiveFoldCV(workable_samples.T,workable_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalTopk=U[:,0:10]\n",
    "new_dataset=finalTopk.T@centered\n",
    "\n",
    "def scramble(samples,labels):\n",
    "    fix_labels=np.reshape(labels,(1,labels.shape[0]))\n",
    "    mashed = np.vstack((samples,fix_labels)).T\n",
    "    np.random.shuffle(mashed)\n",
    "    labels_out = mashed.T[mashed.T.shape[0]-1]\n",
    "    samples_out = mashed.T[0:mashed.T.shape[0]-1]\n",
    "    return [samples_out,labels_out]\n",
    "    \n",
    "workable_samples, workable_labels = scramble(new_dataset,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(mega_confusion)):\n",
    "    print(\"k (nearest neighbors)\",((2*(i+1))+1),\"with confusion matrix\\n\",mega_confusion[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3\n",
    "for matrix in mega_confusion:\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i in range(5):\n",
    "        for j in range(5):\n",
    "            if(i==j):\n",
    "                correct+=matrix[i][j]\n",
    "            total += matrix[i][j]\n",
    "    print(k, \"with correct classification percentage\", (correct/total)*100)\n",
    "    k += 2\n",
    "                            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine (Lester Fan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeScatterPlot(title, pointsLists, colors, markers, xlabel = '', ylabel = ''):\n",
    "    # Create plot\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "     \n",
    "    for i, pointsList in enumerate(pointsLists):\n",
    "        currColor = colors[i]\n",
    "        currMarker = markers[i]\n",
    "        for point in pointsList:\n",
    "            x, y = point\n",
    "            ax.scatter(x, y, c=currColor, marker=currMarker)\n",
    "        \n",
    "    # Temporary, add line\n",
    "    axes = plt.gca()\n",
    "    x_vals = np.array(axes.get_xlim())\n",
    "    y_vals = 0 + 1 * x_vals\n",
    "    plt.plot(x_vals, y_vals, '--')\n",
    "    \n",
    "    # plt.xlim(0, 100)\n",
    "    # plt.ylim(0, 100)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.legend(loc=2)\n",
    "    plt.show()\n",
    "\n",
    "def calcError(y1, y2):\n",
    "    return 1 if y1 != y2 else 0\n",
    "\n",
    "def KCVsplit(k, X, y):\n",
    "    \"\"\"\n",
    "    Splits the training data (X) into k folds in which one fold is used for testing\n",
    "    and all the others are used for training. Does this k times. \n",
    "    \n",
    "    :param int k\n",
    "    :param np.array X\n",
    "    :param np.array y\n",
    "    :return: (folds, Xlist, ylist, table) where folds is a list of all \n",
    "             the folds, Xlist is a list of np.arrays\n",
    "             corresponding to the X used for training in each iteration \n",
    "             (all the folds except for index i),\n",
    "             ylist is a list of np.arrays corresponding to the y used for training\n",
    "             and table is a hash table mapping rows in X to their corresponding\n",
    "             label y.\n",
    "    :rtype: tuple of (list of np.array, np.array, np.array, dict)\n",
    "    \"\"\"\n",
    "    Xlist = []\n",
    "    ylist = []\n",
    "    table = {}\n",
    "    \n",
    "    # Set table up\n",
    "    for i, row in enumerate(X):\n",
    "        table[str(row)] = y[i]\n",
    "    \n",
    "    # Split X up into folds\n",
    "    stepSize = int(X.shape[0] / k) + 1\n",
    "    steps = [min(X.shape[0], stepSize * n) for n in range(1, k)]\n",
    "    folds = np.split(X, steps)\n",
    "    foldLabels = np.split(y, steps)\n",
    "    \n",
    "    for i, currFold in enumerate(folds):\n",
    "        # Get XTrain and yTrain with all the other folds except currFold\n",
    "        otherFolds = [otherFold for j, otherFold in enumerate(folds) if j != i]\n",
    "        otherFoldLabels = [label for j, label in enumerate(foldLabels) if j != i]\n",
    "        XTrain = otherFolds[0]\n",
    "        yTrain = otherFoldLabels[0]\n",
    "        for j in range(1, len(otherFolds)):\n",
    "            XTrain = np.append(XTrain, otherFolds[j], axis=0)\n",
    "            yTrain = np.append(yTrain, otherFoldLabels[j], axis=0)\n",
    "        Xlist.append(XTrain)\n",
    "        ylist.append(yTrain)\n",
    "    \n",
    "    return (folds, Xlist, ylist, table)\n",
    "\n",
    "def performValidation(k, X, y, model, plotDiffs = False):\n",
    "    \"\"\"\n",
    "    Performs cross validation\n",
    "    \"\"\"\n",
    "    folds, Xlist, ylist, table = KCVsplit(k, X, y)\n",
    "    \n",
    "    confusionMatrices = []\n",
    "    foldSizes = []\n",
    "    \n",
    "    # For each fold i, use i as testing, and use all the other\n",
    "    # i - 1 folds for training. Calculate the errors for each and store\n",
    "    # them in avgErrors\n",
    "    predTable = {}\n",
    "    avgErrors = []\n",
    "    for i in range(len(Xlist)):\n",
    "        currFold = folds[i]\n",
    "        XTrain = Xlist[i]\n",
    "        yTrain = ylist[i]\n",
    "        \n",
    "        # Make a confusion matrix\n",
    "        confusionMatrix = [[0 for c in range(5)] for r in range(5)]\n",
    "        \n",
    "        # Train the model\n",
    "        model.fit(XTrain, yTrain)\n",
    "        \n",
    "        # Now, evaluate the trained model against the actual labels,\n",
    "        # keep track of total error!\n",
    "        totalError = 0\n",
    "        for row in currFold:\n",
    "            \n",
    "            # Get the current predicted value\n",
    "            currRow = row.reshape(1, -1)\n",
    "            yPredicted = model.predict(currRow)\n",
    "            \n",
    "            predicted = yPredicted[0]\n",
    "            actual = table[str(row)][0]\n",
    "            \n",
    "            # Update the confusion matrix\n",
    "            confusionMatrix[int(predicted)][int(actual)] += 1\n",
    "            \n",
    "            totalError += calcError(predicted, actual)\n",
    "            predTable[str(row)] = yPredicted\n",
    "        # totalError /= len(currFold)\n",
    "        avgErrors.append(totalError)\n",
    "        foldSizes.append(len(currFold))\n",
    "        confusionMatrices.append(np.array(confusionMatrix))\n",
    "          \n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    plotPoints = []\n",
    "    for row in X:\n",
    "        hashVal = str(row)\n",
    "        predictions.append(predTable[hashVal][0])\n",
    "        actuals.append(table[hashVal][0])\n",
    "    plotPoints = [(predictions[i], actuals[i]) for i in range(len(actuals))]\n",
    "\n",
    "    # Optionally, plot the measured vs predicted values\n",
    "    if plotDiffs:\n",
    "        makeScatterPlot(\"Measured vs Predicted\", \n",
    "                        [plotPoints], \n",
    "                        ['r'], \n",
    "                        ['.'],\n",
    "                        'Predicted',\n",
    "                        'Actual')\n",
    "    return avgErrors, foldSizes, confusionMatrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4321, 10000)\n",
      "(4321, 10000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "model = SVC()\n",
    "print(dataset.shape)\n",
    "pcaData = PCA(dataset.T, 100).T\n",
    "pcaData = dataset\n",
    "print(pcaData.shape)\n",
    "k = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcaData, labels = unisonShuffledCopies(pcaData, labels)\n",
    "errors, foldSizes, confusionMatrices = performValidation(k, pcaData, labels, model)\n",
    "for i, confusionMatrix in enumerate(confusionMatrices):\n",
    "    print(\"Confusion Matrix {}\".format(i))\n",
    "    print(confusionMatrix)\n",
    "    print(\"Accuracy rate = {}\".format(1 - float(errors[i]) / foldSizes[i]))\n",
    "    print(\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Classifier (Anton Maliev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy.linalg as la;\n",
    "def Gaussian(x, mean, cov, dim) :\n",
    "    return 1/np.sqrt((2*np.pi)**dim*la.norm(x))*np.exp(-np.matmul(np.matmul(np.transpose(x-mean),np.inv(cov)),x-mean)/2);\n",
    "def NBClassifier(training, trlabels, test, dim) :\n",
    "    # Split training data and calculate means and standard deviations\n",
    "    train = []; means = []; stds = [];\n",
    "    for i in range(5) :\n",
    "        train.append(training[np.ix_((0,1),np.where(trlabels==0)[1])]);\n",
    "        means.append(train[i].mean(1));\n",
    "        stds.append(train[i].std(1));\n",
    "#     means = np.concatenate((np.concatenate((train0.mean(1),train1.mean(1)),1),train2.mean(1)),1);\n",
    "#     stds = np.concatenate((np.concatenate((train0.std(1),train1.std(1)),1),train2.std(1)),1);\n",
    "    # Classify each test datum based on Gaussian output\n",
    "    result = [];\n",
    "    for i in range(len(result)) :\n",
    "        x = [];\n",
    "        for j in range(dim) :\n",
    "            x.append(test[j,i]);\n",
    "#         x1 = test[0,i];\n",
    "#         x2 = test[1,i];\n",
    "        prob = [];\n",
    "        for j in range(5) :\n",
    "            gauss = 1;\n",
    "            for k in range(dim) :\n",
    "                gauss *= Gaussian(x[k],means[k,j],stds[k,j],np.shape(x)[1])\n",
    "            prob.append();\n",
    "#         prob[0] = Gaussian(x1,means[0,0],stds[0,0]) * Gaussian(x2,means[1,0],stds[1,0]);\n",
    "#         prob[1] = Gaussian(x1,means[0,1],stds[0,1]) * Gaussian(x2,means[1,1],stds[1,1]);\n",
    "#         prob[2] = Gaussian(x1,means[0,2],stds[0,2]) * Gaussian(x2,means[1,2],stds[1,2]);\n",
    "        result.append(np.argmax(prob));\n",
    "    return result;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4321, 10000) (4321, 1)\n",
      "0\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for array",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-aed1283f6016>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mtrs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpcad\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mM\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mfolds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpcad\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mM\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfolds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfolds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0mlbs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpcad\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mfolds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpcad\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfolds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfolds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mtst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpcad\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mM\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfolds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mfolds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for array"
     ]
    }
   ],
   "source": [
    "pcad, labels = unisonShuffledCopies(pcaData, labels);\n",
    "print(np.shape(pcad),np.shape(labels));\n",
    "\n",
    "N = np.shape(pcad)[0];\n",
    "M = np.shape(pcad)[1];\n",
    "folds = [0,N//5,2*N//5,3*N//5,4*N//5,N];\n",
    "\n",
    "kcross = [];\n",
    "for i in range(5) :\n",
    "    print(i);\n",
    "    trs = np.concatenate((pcad[0:M,0:folds[i+1]],pcad[0:M,folds[i+1]:folds[5]]),1);\n",
    "    lbs = np.concatenate((pcad[0:folds[i+1]],pcad[folds[i+1],folds[5]]),1);\n",
    "    tst = pcad[0:M,folds[i]:folds[i+1]];\n",
    "    kcross.append(NBClassifier(trs,lbs,tst));\n",
    "\n",
    "# kcross1 = NBClassifier(feat2a[0:2,36:178],feat2a[2,36:178],feat2a[0:2,0:36]);\n",
    "# train2 = np.concatenate((feat2a[0:2,0:36],feat2a[0:2,72:178]),1);\n",
    "# labels2 = np.concatenate((feat2a[2,0:36],feat2a[2,72:178]),1);\n",
    "# kcross2 = NBClassifier(train2,labels2,feat2a[0:2,36:72]);\n",
    "# train3 = np.concatenate((feat2a[0:2,0:72],feat2a[0:2,108:178]),1);\n",
    "# labels3 = np.concatenate((feat2a[2,0:72],feat2a[2,108:178]),1);\n",
    "# kcross3 = NBClassifier(train3,labels3,feat2a[0:2,72:108]);\n",
    "# train4 = np.concatenate((feat2a[0:2,0:108],feat2a[0:2,143:178]),1);\n",
    "# labels4 = np.concatenate((feat2a[2,0:108],feat2a[2,143:178]),1);\n",
    "# kcross4 = NBClassifier(train4,labels4,feat2a[0:2,108:143]);\n",
    "# kcross5 = NBClassifier(feat2a[0:2,0:143],feat2a[2,0:143],feat2a[0:2,143:178]);\n",
    "\n",
    "accs = [];\n",
    "for i in range(5) :\n",
    "    accs.append(len(np.where(kcross[i]-feat2a[2,folds[i]:folds[i+1]] == 0)[1])/(folds[i+1]-folds[i]));\n",
    "#     accs.append(len(np.where(kcross[i]-feat2a[2,36:72] == 0)[1])/36);\n",
    "#     accs.append(len(np.where(kcross[i]-feat2a[2,72:108] == 0)[1])/36);\n",
    "#     accs.append(len(np.where(kcross[i]-feat2a[2,108:143] == 0)[1])/36);\n",
    "#     accs.append(len(np.where(kcross[i]-feat2a[2,143:178] == 0)[1])/36);\n",
    "\n",
    "print(\"The accuracies for the five folds are: \",accs);\n",
    "print(\"Mean: \",np.mean(accs));\n",
    "print(\"Standard deviation: \",np.std(accs));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
